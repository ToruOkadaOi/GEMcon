{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp import ClientTimeout\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import yaml\n",
    "from rich.progress import (\n",
    "    Progress, BarColumn, TimeElapsedColumn,\n",
    "    TimeRemainingColumn, DownloadColumn, TransferSpeedColumn\n",
    ")\n",
    "timeout = ClientTimeout(total=None)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "logger = logging.getLogger(\"paxdb\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_links():\n",
    "    base = \"https://pax-db.org\"\n",
    "    listing_url = f\"{base}/downloads/3.0/datasets/9606/\"\n",
    "\n",
    "    r = requests.get(listing_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.endswith(\".txt\"):\n",
    "            links.append(urljoin(listing_url, href))\n",
    "\n",
    "    return links\n",
    "\n",
    "# download func; In 1 mb chunks\n",
    "async def download_file(url, filename, chunk_size=1024*1024, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            connector = aiohttp.TCPConnector(limit=10)\n",
    "            async with aiohttp.ClientSession(timeout=timeout, headers=headers, connector=connector) as session:\n",
    "                async with session.get(url) as response:\n",
    "\n",
    "                    if response.status != 200:\n",
    "                        logger.error(\"HTTP error\", extra={\"status\": response.status, \"url\": url})\n",
    "                        raise Exception(f\"Failed: {response.status}\")\n",
    "                    \n",
    "                    total = response.content_length\n",
    "\n",
    "                    from rich.progress import (\n",
    "                        Progress, BarColumn, TimeElapsedColumn,\n",
    "                        TimeRemainingColumn, DownloadColumn, TransferSpeedColumn\n",
    "                    )\n",
    "\n",
    "                    with Progress(\n",
    "                        \"[progress.description]{task.description}\",\n",
    "                        DownloadColumn(),\n",
    "                        BarColumn(),\n",
    "                        TransferSpeedColumn(),\n",
    "                        TimeRemainingColumn(),\n",
    "                        TimeElapsedColumn(),\n",
    "                    ) as progress:\n",
    "\n",
    "                        task = progress.add_task(\n",
    "                            f\"Downloading {os.path.basename(filename)}\",\n",
    "                            total=total\n",
    "                        )\n",
    "\n",
    "                        with open(filename, \"wb\") as f:\n",
    "                            async for chunk in response.content.iter_chunked(chunk_size):\n",
    "                                try:\n",
    "                                    f.write(chunk)\n",
    "                                    #raise RuntimeError(\"test\")\n",
    "                                    progress.update(task, advance=len(chunk))\n",
    "                                except Exception:\n",
    "                                    logger.critical(\"failed to write chunks while downloading\", exc_info=True)\n",
    "                                    raise\n",
    "\n",
    "            print(\"Download complete:\", filename)\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {i+1} failed:\", e)\n",
    "            await asyncio.sleep(2 ** i)\n",
    "save_dir = \"data/data_raw/paxdb_human_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "async def download_all(urls):\n",
    "    tasks = [] # to store the coroutines\n",
    "    for url in urls:\n",
    "        filename = os.path.join(save_dir, os.path.basename(url))\n",
    "        tasks.append(download_file(url, filename)) # use the previous download func.\n",
    "    \n",
    "    await asyncio.gather(*tasks) # unpack cor. list\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     urls = get_links()\n",
    "#     await(download_all(urls)) # asyncio.run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pax_aman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
